{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFEXH_iqDLEg"
      },
      "source": [
        "### no noisy hallway environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkqC615Z66nL"
      },
      "source": [
        "# our method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l6RzrHMO68NM",
        "outputId": "18cc2c45-71c1-4f07-9073-7406dd4a363d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "import math\n",
        "import imageio\n",
        "\n",
        "# Check for CUDA availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class CNNFeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(CNNFeatureExtractor, self).__init__()\n",
        "\n",
        "        # Input shape: (channels, height, width)\n",
        "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "\n",
        "        # Calculate size after convolutions\n",
        "        def conv2d_size_out(size, kernel_size, stride):\n",
        "            return (size - kernel_size) // stride + 1\n",
        "\n",
        "        conv_width = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_shape[2], 8, 4), 4, 2), 3, 1)\n",
        "        conv_height = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_shape[1], 8, 4), 4, 2), 3, 1)\n",
        "        self.feature_size = conv_width * conv_height * 64\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Preprocess image\n",
        "        x = x / 255.0  # Normalize pixel values\n",
        "        x = x.permute(0, 3, 1, 2)  # Change from (N, H, W, C) to (N, C, H, W)\n",
        "\n",
        "        # Convolutional layers\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.reshape(x.size(0), -1)  # Use reshape instead of view\n",
        "\n",
        "        return x\n",
        "\n",
        "class MSEPredictionModel(nn.Module):\n",
        "    \"\"\"Primary network for predicting the next state using MSE loss\"\"\"\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(MSEPredictionModel, self).__init__()\n",
        "\n",
        "        # Store input shape\n",
        "        self.input_shape = input_shape  # (channels, height, width)\n",
        "        self.height = input_shape[1]\n",
        "        self.width = input_shape[2]\n",
        "        self.channels = input_shape[0]\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # Feature extractor\n",
        "        self.feature_extractor = CNNFeatureExtractor(input_shape)\n",
        "        feature_size = self.feature_extractor.feature_size\n",
        "\n",
        "        # Forward model to combine state features and action\n",
        "        self.forward_model = nn.Sequential(\n",
        "            nn.Linear(feature_size + num_actions, 512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder to predict next state\n",
        "        h_out = self.height // 8  # Estimate based on typical CNN feature size reduction\n",
        "        w_out = self.width // 8\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(512, h_out * w_out * 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (64, h_out, w_out)),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, self.channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Sigmoid(),  # Output in range [0,1]\n",
        "            nn.Upsample(size=(self.height, self.width), mode='bilinear', align_corners=False)\n",
        "        )\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \"\"\"Forward pass to predict next state\"\"\"\n",
        "        # Extract features\n",
        "        state_features = self.feature_extractor(state)\n",
        "\n",
        "        # Create one-hot encoding of action\n",
        "        action_one_hot = F.one_hot(action, num_classes=self.num_actions).float()\n",
        "\n",
        "        # Combine features and action\n",
        "        combined = torch.cat([state_features, action_one_hot], dim=1)\n",
        "        features = self.forward_model(combined)\n",
        "\n",
        "        # Predict next state\n",
        "        predicted_next_state = self.decoder(features)\n",
        "\n",
        "        return predicted_next_state\n",
        "\n",
        "    def update(self, states, next_states, actions):\n",
        "        \"\"\"Update the prediction model using MSE loss\"\"\"\n",
        "        # Forward pass\n",
        "        predicted_next_states = self.forward(states, actions)\n",
        "\n",
        "        # Target state (normalized to match prediction format)\n",
        "        target_next_states = next_states.permute(0, 3, 1, 2) / 255.0\n",
        "\n",
        "        # Compute MSE loss\n",
        "        loss = F.mse_loss(predicted_next_states, target_next_states)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def compute_error(self, state, next_state, action):\n",
        "        \"\"\"Compute prediction error for a given state-action-next_state triplet\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
        "        action_tensor = torch.LongTensor([action]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get prediction\n",
        "            predicted_next_state = self.forward(state_tensor, action_tensor)\n",
        "\n",
        "            # Convert target to match prediction format\n",
        "            next_state_tensor_permuted = next_state_tensor.permute(0, 3, 1, 2) / 255.0\n",
        "\n",
        "            # Calculate squared error - per element\n",
        "            squared_error = torch.pow(next_state_tensor_permuted - predicted_next_state, 2)\n",
        "\n",
        "            # Mean over all dimensions to get scalar error\n",
        "            mse = squared_error.mean().item()\n",
        "\n",
        "        return mse\n",
        "\n",
        "class UncertaintyPredictionModel(nn.Module):\n",
        "    \"\"\"Separate network for predicting expected prediction errors\"\"\"\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(UncertaintyPredictionModel, self).__init__()\n",
        "\n",
        "        # Feature extractor (same architecture as main model)\n",
        "        self.feature_extractor = CNNFeatureExtractor(input_shape)\n",
        "        feature_size = self.feature_extractor.feature_size\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # Network to predict log of expected error\n",
        "        self.uncertainty_net = nn.Sequential(\n",
        "            nn.Linear(feature_size + num_actions, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)  # Predict a single scalar - log of expected MSE\n",
        "        )\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=0.01)  # Higher learning rate as it's a simpler task\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \"\"\"Forward pass to predict log of expected prediction error\"\"\"\n",
        "        # Extract features\n",
        "        state_features = self.feature_extractor(state)\n",
        "\n",
        "        # Create one-hot encoding of action\n",
        "        action_one_hot = F.one_hot(action, num_classes=self.num_actions).float()\n",
        "\n",
        "        # Combine features and action\n",
        "        combined = torch.cat([state_features, action_one_hot], dim=1)\n",
        "\n",
        "        # Predict log uncertainty\n",
        "        log_uncertainty = self.uncertainty_net(combined)\n",
        "\n",
        "        # Clamp to avoid numerical issues\n",
        "        log_uncertainty = torch.clamp(log_uncertainty, min=-10.0, max=10.0)\n",
        "\n",
        "        return log_uncertainty\n",
        "\n",
        "    def update(self, states, actions, actual_errors):\n",
        "        \"\"\"Update uncertainty predictor to better predict errors\"\"\"\n",
        "        # Forward pass\n",
        "        log_predicted_errors = self.forward(states, actions)\n",
        "\n",
        "        # Convert actual errors to tensor and add epsilon to avoid log(0)\n",
        "        epsilon = 1e-6\n",
        "        log_actual_errors = torch.log(torch.FloatTensor(actual_errors).to(device) + epsilon).unsqueeze(1)\n",
        "\n",
        "        # Compute MSE loss between log errors\n",
        "        loss = F.mse_loss(log_predicted_errors, log_actual_errors)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "class LearningProgressCuriosity:\n",
        "    \"\"\"Combined model that measures curiosity as improvement over expected error\"\"\"\n",
        "    def __init__(self, input_shape, num_actions, eta=1.0, uncertainty_buffer_size=100,\n",
        "                 update_uncertainty_every=5):\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "        self.eta = eta  # Weighting for uncertainty-adjusted reward\n",
        "\n",
        "        # Main prediction model\n",
        "        self.prediction_model = MSEPredictionModel(input_shape, num_actions).to(device)\n",
        "\n",
        "        # Uncertainty prediction model\n",
        "        self.uncertainty_model = UncertaintyPredictionModel(input_shape, num_actions).to(device)\n",
        "\n",
        "        # Buffer for storing recent prediction errors\n",
        "        self.error_buffer = []\n",
        "        self.buffer_size = uncertainty_buffer_size\n",
        "\n",
        "        # Update frequency for uncertainty model\n",
        "        self.update_uncertainty_every = update_uncertainty_every\n",
        "        self.steps_since_uncertainty_update = 0\n",
        "\n",
        "    def get_intrinsic_reward(self, state, next_state, action):\n",
        "        \"\"\"Calculate intrinsic reward based on improvement over expected error\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        action_tensor = torch.LongTensor([action]).to(device)\n",
        "\n",
        "        # Get actual MSE for this transition\n",
        "        actual_mse = self.prediction_model.compute_error(state, next_state, action)\n",
        "\n",
        "        # Add error to buffer for later uncertainty training\n",
        "        self.error_buffer.append((state, action, actual_mse))\n",
        "        if len(self.error_buffer) > self.buffer_size:\n",
        "            self.error_buffer.pop(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Predict log of expected error\n",
        "            log_expected_error = self.uncertainty_model(state_tensor, action_tensor)\n",
        "\n",
        "            # Convert to expected error\n",
        "            expected_error = torch.exp(log_expected_error).item()\n",
        "\n",
        "            # Calculate improvement-based reward:\n",
        "            # Positive when actual error is less than expected (improvement)\n",
        "            # Negative when actual error is more than expected (getting worse)\n",
        "            intrinsic_reward = self.eta * expected_error - actual_mse\n",
        "\n",
        "            # Clip to not exceed 1 (rather than clipping to be non-negative)\n",
        "            intrinsic_reward = min(0.5, intrinsic_reward)\n",
        "\n",
        "        return intrinsic_reward\n",
        "\n",
        "    def update(self, states, next_states, actions):\n",
        "        \"\"\"Update both models\"\"\"\n",
        "        # Always update prediction model\n",
        "        pred_loss = self.prediction_model.update(states, next_states, actions)\n",
        "\n",
        "        # Update uncertainty model periodically\n",
        "        self.steps_since_uncertainty_update += 1\n",
        "        uncertainty_loss = 0.0\n",
        "\n",
        "        if self.steps_since_uncertainty_update >= self.update_uncertainty_every and len(self.error_buffer) > 0:\n",
        "            # Sample from error buffer (up to batch_size)\n",
        "            batch_size = min(32, len(self.error_buffer))\n",
        "            indices = np.random.choice(len(self.error_buffer), batch_size, replace=False)\n",
        "\n",
        "            sampled_states = []\n",
        "            sampled_actions = []\n",
        "            sampled_errors = []\n",
        "\n",
        "            for idx in indices:\n",
        "                state, action, error = self.error_buffer[idx]\n",
        "                sampled_states.append(state)\n",
        "                sampled_actions.append(action)\n",
        "                sampled_errors.append(error)\n",
        "\n",
        "            # Convert to tensors\n",
        "            state_batch = torch.FloatTensor(np.array(sampled_states)).to(device)\n",
        "            action_batch = torch.LongTensor(sampled_actions).to(device)\n",
        "\n",
        "            # Update uncertainty model\n",
        "            uncertainty_loss = self.uncertainty_model.update(state_batch, action_batch, sampled_errors)\n",
        "\n",
        "            # Reset counter\n",
        "            self.steps_since_uncertainty_update = 0\n",
        "\n",
        "        return pred_loss, uncertainty_loss\n",
        "\n",
        "class A2CNetwork(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(A2CNetwork, self).__init__()\n",
        "\n",
        "        # Feature extractor\n",
        "        self.feature_extractor = CNNFeatureExtractor(input_shape)\n",
        "        feature_size = self.feature_extractor.feature_size\n",
        "\n",
        "        # Actor (policy) and critic (value) heads\n",
        "        self.fc_actor = nn.Sequential(\n",
        "            nn.Linear(feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_actions)\n",
        "        )\n",
        "\n",
        "        self.fc_critic = nn.Sequential(\n",
        "            nn.Linear(feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features\n",
        "        features = self.feature_extractor(x)\n",
        "\n",
        "        # Actor (policy) head\n",
        "        action_logits = self.fc_actor(features)\n",
        "\n",
        "        # Critic (value) head\n",
        "        value = self.fc_critic(features)\n",
        "\n",
        "        return action_logits, value\n",
        "\n",
        "# Memory class for experience collection\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.intrinsic_rewards = []\n",
        "        self.next_states = []\n",
        "        self.dones = []\n",
        "        self.logprobs = []\n",
        "        self.values = []\n",
        "\n",
        "    def add(self, state, action, reward, intrinsic_reward, next_state, done, logprob, value):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.intrinsic_rewards.append(intrinsic_reward)\n",
        "        self.next_states.append(next_state)\n",
        "        self.dones.append(done)\n",
        "        self.logprobs.append(logprob)\n",
        "        self.values.append(value)\n",
        "\n",
        "    def clear(self):\n",
        "        self.states.clear()\n",
        "        self.actions.clear()\n",
        "        self.rewards.clear()\n",
        "        self.intrinsic_rewards.clear()\n",
        "        self.next_states.clear()\n",
        "        self.dones.clear()\n",
        "        self.logprobs.clear()\n",
        "        self.values.clear()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.states)\n",
        "\n",
        "class A2CAgentWithLearningProgressCuriosity:\n",
        "    def __init__(self, env, learning_rate=0.0003, gamma=0.99, gamma_intrinsic=0.99,\n",
        "                 lambda_intrinsic=1.0, gae_lambda=0.95, entropy_coef=0.01,\n",
        "                 value_loss_coef=0.5, max_grad_norm=0.5):\n",
        "\n",
        "        # Environment info\n",
        "        self.env = env\n",
        "        self.obs_shape = env.observation_space.shape\n",
        "        self.num_actions = env.action_space.n\n",
        "\n",
        "        # Add persistent visited states tracking\n",
        "        self.visited_states_grid = np.zeros((18*4, 12*4))  # Match the environment's grid size\n",
        "        self.total_visited_states = 0\n",
        "\n",
        "        # Learning Progress curiosity model\n",
        "        self.curiosity = LearningProgressCuriosity(\n",
        "            (self.obs_shape[2], self.obs_shape[0], self.obs_shape[1]),\n",
        "            self.num_actions,\n",
        "            eta=1.0,\n",
        "            uncertainty_buffer_size=100,\n",
        "            update_uncertainty_every=5\n",
        "        )\n",
        "\n",
        "        # Policy network\n",
        "        self.policy = A2CNetwork(\n",
        "            (self.obs_shape[2], self.obs_shape[0], self.obs_shape[1]),\n",
        "            self.num_actions).to(device)\n",
        "\n",
        "        # A2C uses RMSprop optimizer\n",
        "        self.optimizer = optim.RMSprop(self.policy.parameters(), lr=learning_rate,\n",
        "                                      alpha=0.99, eps=1e-8)\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.gamma = gamma\n",
        "        self.gamma_intrinsic = gamma_intrinsic\n",
        "        self.lambda_intrinsic = lambda_intrinsic\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.value_loss_coef = value_loss_coef\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "\n",
        "        # Memory\n",
        "        self.memory = Memory()\n",
        "\n",
        "        # Training stats\n",
        "        self.exploration_steps = 0\n",
        "        self.first_success_step = None\n",
        "\n",
        "    def update_visited_states(self, info):\n",
        "        \"\"\"\n",
        "        Update our own persistent tracking of visited states\n",
        "        Returns the total number of visited states so far\n",
        "        \"\"\"\n",
        "        # Extract agent position from info if available, otherwise try to get it from environment\n",
        "        if hasattr(self.env, 'agent') and hasattr(self.env.agent, 'pos'):\n",
        "            agent_x = self.env.agent.pos[0]\n",
        "            agent_z = self.env.agent.pos[2]\n",
        "\n",
        "            # Convert to grid coordinates\n",
        "            grid_x = int(agent_x * 4)\n",
        "            grid_z = int(agent_z * 4)\n",
        "\n",
        "            # Check bounds to prevent index errors\n",
        "            if 0 <= grid_x < 18*4 and 0 <= grid_z < 12*4:\n",
        "                # If this is a new state, increment counter\n",
        "                if self.visited_states_grid[grid_x, grid_z] == 0:\n",
        "                    self.visited_states_grid[grid_x, grid_z] = 1\n",
        "                    self.total_visited_states += 1\n",
        "\n",
        "        return self.total_visited_states\n",
        "\n",
        "    def select_action(self, state, training=True):\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action_logits, value = self.policy(state_tensor)\n",
        "            action_probs = F.softmax(action_logits, dim=-1)\n",
        "            dist = Categorical(action_probs)\n",
        "            action = dist.sample().item()\n",
        "            logprob = dist.log_prob(torch.tensor(action).to(device)).item()\n",
        "            value = value.item()\n",
        "\n",
        "        return action, logprob, value\n",
        "\n",
        "    def compute_gae(self, rewards, values, dones, next_value=0):\n",
        "        \"\"\"Compute Generalized Advantage Estimation (GAE)\"\"\"\n",
        "        returns = []\n",
        "        gae = 0\n",
        "\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            delta = rewards[i] + self.gamma * next_value * (1 - dones[i]) - values[i]\n",
        "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[i]) * gae\n",
        "            next_value = values[i]\n",
        "            returns.insert(0, gae + values[i])\n",
        "\n",
        "        return returns\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"A2C update method\"\"\"\n",
        "        # If memory is empty, nothing to update\n",
        "        if len(self.memory.states) == 0:\n",
        "            return 0, 0\n",
        "\n",
        "        # Convert lists to tensors\n",
        "        states = torch.FloatTensor(np.array(self.memory.states)).to(device)\n",
        "        next_states = torch.FloatTensor(np.array(self.memory.next_states)).to(device)\n",
        "        actions = torch.LongTensor(self.memory.actions).to(device)\n",
        "        values = torch.FloatTensor(self.memory.values).to(device)\n",
        "        dones = np.array(self.memory.dones)\n",
        "\n",
        "        # Get combined rewards\n",
        "        extrinsic_rewards = np.array(self.memory.rewards)\n",
        "        intrinsic_rewards = np.array(self.memory.intrinsic_rewards)\n",
        "\n",
        "        # Update curiosity model\n",
        "        pred_loss, uncertainty_loss = self.curiosity.update(states, next_states, actions)\n",
        "\n",
        "        # Combine rewards\n",
        "        combined_rewards = extrinsic_rewards + (self.lambda_intrinsic * intrinsic_rewards)\n",
        "\n",
        "        # Ensure rewards do not exceed 1 (rather than clipping to be non-negative)\n",
        "        combined_rewards = np.minimum(combined_rewards, 1.0)\n",
        "\n",
        "        # Compute returns with GAE\n",
        "        with torch.no_grad():\n",
        "            # Get value of the last state\n",
        "            if len(self.memory.next_states) > 0:\n",
        "                last_state = torch.FloatTensor(self.memory.next_states[-1]).unsqueeze(0).to(device)\n",
        "                _, last_value = self.policy(last_state)\n",
        "                last_value = last_value.item()\n",
        "            else:\n",
        "                last_value = 0\n",
        "\n",
        "        returns = torch.FloatTensor(self.compute_gae(combined_rewards, self.memory.values, dones, last_value)).to(device)\n",
        "\n",
        "        # Normalize returns\n",
        "        if len(returns) > 1:\n",
        "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        # Calculate updated policy and value\n",
        "        action_logits, state_values = self.policy(states)\n",
        "        action_probs = F.softmax(action_logits, dim=-1)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        # Calculate advantage\n",
        "        advantages = returns - state_values.squeeze()\n",
        "\n",
        "        # Calculate policy loss\n",
        "        policy_loss = -(dist.log_prob(actions) * advantages.detach()).mean()\n",
        "\n",
        "        # Calculate value loss\n",
        "        value_loss = F.mse_loss(state_values.squeeze(), returns)\n",
        "\n",
        "        # Calculate entropy for exploration\n",
        "        entropy = dist.entropy().mean()\n",
        "\n",
        "        # Total loss (A2C style)\n",
        "        loss = policy_loss + self.value_loss_coef * value_loss - self.entropy_coef * entropy\n",
        "\n",
        "        # Update policy\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Clear memory\n",
        "        self.memory.clear()\n",
        "\n",
        "        return pred_loss, uncertainty_loss\n",
        "\n",
        "    def train(self, max_exploration_steps=10000, update_frequency=512, log_frequency=500,\n",
        "              save_frequency=1000, frame_capture_frequency=20):\n",
        "        \"\"\"\n",
        "        Train with a single continuous exploration without episodic resets.\n",
        "        Continue exploring for the full number of steps, even if goal is found.\n",
        "\n",
        "        Args:\n",
        "            max_exploration_steps: Maximum number of steps to explore\n",
        "            update_frequency: How often to update the policy\n",
        "            log_frequency: How often to log progress\n",
        "            save_frequency: How often to save the model\n",
        "            frame_capture_frequency: How often to capture frames for the GIF (every N steps)\n",
        "\n",
        "        Returns:\n",
        "            total_reward: Total reward accumulated\n",
        "            first_success_step: Step at which first success occurred\n",
        "            visited_states: List of visited states over time\n",
        "            frames: List of frames for creating a GIF\n",
        "            steps_record: List of step numbers for plotting\n",
        "            cumulative_intrinsic_rewards: List of cumulative intrinsic rewards for plotting\n",
        "        \"\"\"\n",
        "        # Initialize environment once\n",
        "        state, info = self.env.reset()\n",
        "\n",
        "        # Initialize visited states tracking with the first state\n",
        "        self.update_visited_states(info)\n",
        "        visited_states = [self.total_visited_states]  # Use our counter instead of info[\"visited_state\"]\n",
        "\n",
        "        # For visualization - collect frames for GIF\n",
        "        frames = []\n",
        "        # Capture initial frame\n",
        "        frames.append(self.env.render_top_view())\n",
        "\n",
        "        # Training statistics\n",
        "        total_reward = 0\n",
        "        total_intrinsic_reward = 0\n",
        "        intrinsic_rewards_history = []\n",
        "        cumulative_intrinsic_rewards = [0]  # Start with 0\n",
        "        steps_record = [0]  # Record steps for plotting\n",
        "        timesteps_since_update = 0\n",
        "        success_reached = False\n",
        "\n",
        "        # Single exploration loop\n",
        "        for step in range(1, max_exploration_steps + 1):\n",
        "            # Select action\n",
        "            action, logprob, value = self.select_action(state)\n",
        "            if step % 100 == 0:  # Log every 100 steps\n",
        "                prob_percentage = math.exp(logprob) * 100  # Convert from log probability to percentage\n",
        "                print(f\"action {action}, Action probability: {prob_percentage:.2f}%\")\n",
        "\n",
        "            # Take action\n",
        "            next_state, reward, termination, truncation, info = self.env.step(action)\n",
        "            done = termination or truncation\n",
        "\n",
        "            # Track visited states using our own method\n",
        "            current_visited = self.update_visited_states(info)\n",
        "            visited_states.append(current_visited)\n",
        "\n",
        "            # Capture frame for GIF every N steps\n",
        "            if step % frame_capture_frequency == 0:\n",
        "                frames.append(self.env.render_top_view())\n",
        "\n",
        "            # Compute intrinsic reward using learning progress\n",
        "            intrinsic_reward = self.curiosity.get_intrinsic_reward(state, next_state, action)\n",
        "            intrinsic_rewards_history.append(intrinsic_reward)\n",
        "\n",
        "            # Track cumulative intrinsic reward for plotting\n",
        "            total_intrinsic_reward += intrinsic_reward\n",
        "            cumulative_intrinsic_rewards.append(total_intrinsic_reward)\n",
        "            steps_record.append(step)\n",
        "\n",
        "            # Store in memory\n",
        "            self.memory.add(state, action, reward, intrinsic_reward, next_state, done, logprob, value)\n",
        "\n",
        "            # Update state\n",
        "            state = next_state\n",
        "            total_reward += reward + intrinsic_reward\n",
        "            timesteps_since_update += 1\n",
        "            self.exploration_steps += 1\n",
        "\n",
        "            if step % log_frequency == 0:\n",
        "                print(f\"Step {step}, intrinsic reward: {intrinsic_reward:.4f}, cumulative: {total_intrinsic_reward:.4f}\")\n",
        "\n",
        "            # Check if goal reached for the first time\n",
        "            if reward > 0 and not success_reached:\n",
        "                success_reached = True\n",
        "                self.first_success_step = step\n",
        "                print(f\"First success at step {self.first_success_step}\")\n",
        "                # But continue exploring rather than stopping\n",
        "\n",
        "            # Update policy if enough timesteps have elapsed\n",
        "            if timesteps_since_update >= update_frequency:\n",
        "                pred_loss, uncertainty_loss = self.update()\n",
        "                timesteps_since_update = 0\n",
        "                print(f\"Step {step}, Visited states: {current_visited}, Pred Loss: {pred_loss:.4f}, Uncertainty Loss: {uncertainty_loss:.4f}\")\n",
        "                # Capture frame after each policy update\n",
        "                frames.append(self.env.render_top_view())\n",
        "\n",
        "            # Save model periodically\n",
        "            if step % save_frequency == 0:\n",
        "                self.save(f\"a2c_lp_curiosity_model_step_{step}.pt\")\n",
        "\n",
        "            # If environment terminated, reset it\n",
        "            if done:\n",
        "                print(f\"Environment terminated at step {step}, resetting...\")\n",
        "                state, info = self.env.reset()\n",
        "                # Capture frame after reset\n",
        "                frames.append(self.env.render_top_view())\n",
        "\n",
        "        # Capture final frame\n",
        "        frames.append(self.env.render_top_view())\n",
        "\n",
        "        return total_reward, self.first_success_step, visited_states, frames, steps_record, cumulative_intrinsic_rewards\n",
        "\n",
        "    def save(self, filename):\n",
        "        torch.save({\n",
        "            'policy_state_dict': self.policy.state_dict(),\n",
        "            'curiosity_prediction_state_dict': self.curiosity.prediction_model.state_dict(),\n",
        "            'curiosity_uncertainty_state_dict': self.curiosity.uncertainty_model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'exploration_steps': self.exploration_steps,\n",
        "            'first_success_step': self.first_success_step\n",
        "        }, filename)\n",
        "\n",
        "    def load(self, filename):\n",
        "        checkpoint = torch.load(filename)\n",
        "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "        self.curiosity.prediction_model.load_state_dict(checkpoint['curiosity_prediction_state_dict'])\n",
        "        self.curiosity.uncertainty_model.load_state_dict(checkpoint['curiosity_uncertainty_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.exploration_steps = checkpoint.get('exploration_steps', 0)\n",
        "        self.first_success_step = checkpoint.get('first_success_step', None)\n",
        "\n",
        "def create_exploration_gif(frames, filename, fps=10):\n",
        "    \"\"\"\n",
        "    Create a GIF from collected frames\n",
        "\n",
        "    Args:\n",
        "        frames: List of frames (numpy arrays)\n",
        "        filename: Output filename\n",
        "        fps: Frames per second in the GIF\n",
        "    \"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "    # Set the duration between frames (in milliseconds)\n",
        "    duration = 1000 / fps\n",
        "\n",
        "    # Create GIF\n",
        "    imageio.mimsave(filename, frames, duration=duration)\n",
        "    print(f\"Created GIF: {filename}\")\n",
        "\n",
        "def plot_intrinsic_rewards(steps, rewards, run_number):\n",
        "    \"\"\"\n",
        "    Plot and display the cumulative intrinsic rewards over time for a single run\n",
        "\n",
        "    Args:\n",
        "        steps: List of step numbers\n",
        "        rewards: List of cumulative intrinsic rewards\n",
        "        run_number: Current run number for labeling\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(steps, rewards, '-b', linewidth=2)\n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel('Cumulative Intrinsic Reward')\n",
        "    plt.title(f'Cumulative Intrinsic Rewards Over Time (Run {run_number})')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Save the figure\n",
        "    plt.savefig(f'intrinsic_rewards_run_{run_number}.png')\n",
        "\n",
        "    # Display the figure\n",
        "    plt.show()\n",
        "\n",
        "    # Close to prevent memory issues with many plots\n",
        "    plt.close()\n",
        "\n",
        "def evaluate_state_coverage_lp(num_runs=8, max_steps=5000, log_frequency=100, frame_capture_frequency=20):\n",
        "    \"\"\"\n",
        "    Run multiple training sessions from scratch and track state coverage over time using Learning Progress agent\n",
        "\n",
        "    Args:\n",
        "        num_runs: Number of independent training runs\n",
        "        max_steps: Maximum number of exploration steps per run\n",
        "        log_frequency: How often to log data points\n",
        "        frame_capture_frequency: How often to capture frames for the GIF\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with evaluation data including visited states over time\n",
        "    \"\"\"\n",
        "    # Storage for visited state data\n",
        "    all_visited_states = []\n",
        "    all_intrinsic_rewards = []\n",
        "    all_steps = []\n",
        "\n",
        "    # Create output directories if they don't exist\n",
        "    os.makedirs(\"evaluation_data_lp\", exist_ok=True)\n",
        "    os.makedirs(\"visualization_lp\", exist_ok=True)\n",
        "\n",
        "    for run in range(1, num_runs + 1):\n",
        "        print(f\"\\n--- Starting Run {run}/{num_runs} with Learning Progress Curiosity ---\")\n",
        "\n",
        "        # Create a new environment and agent for each run\n",
        "        env = MazeEnv(render_mode=None)\n",
        "        agent = A2CAgentWithLearningProgressCuriosity(\n",
        "            env,\n",
        "            learning_rate=0.01,\n",
        "            gamma=0.99,\n",
        "            lambda_intrinsic=0.1,\n",
        "            entropy_coef=0.05,\n",
        "            value_loss_coef=0.5,\n",
        "            max_grad_norm=0.5\n",
        "        )\n",
        "\n",
        "        # Train with continuous exploration and collect visited states, frames, and intrinsic rewards\n",
        "        _, _, visited_states, frames, steps_record, cumulative_intrinsic_rewards = agent.train(\n",
        "            max_exploration_steps=max_steps,\n",
        "            update_frequency=64,\n",
        "            log_frequency=log_frequency,\n",
        "            frame_capture_frequency=frame_capture_frequency\n",
        "        )\n",
        "\n",
        "        # Plot and display the intrinsic rewards for this run\n",
        "        plot_intrinsic_rewards(steps_record, cumulative_intrinsic_rewards, run)\n",
        "\n",
        "        # Create GIF for this run\n",
        "        create_exploration_gif(frames, f\"visualization_lp/run_{run}_exploration.gif\")\n",
        "\n",
        "        # Save detailed data for this run\n",
        "        run_data = {\n",
        "            \"visited_states\": visited_states,\n",
        "            \"steps\": list(range(len(visited_states))),\n",
        "            \"intrinsic_rewards\": cumulative_intrinsic_rewards,\n",
        "            \"steps_record\": steps_record\n",
        "        }\n",
        "\n",
        "        with open(f\"evaluation_data_lp/run_{run}_data.pkl\", \"wb\") as f:\n",
        "            pickle.dump(run_data, f)\n",
        "\n",
        "        all_visited_states.append(visited_states)\n",
        "        all_intrinsic_rewards.append(cumulative_intrinsic_rewards)\n",
        "        all_steps.append(steps_record)\n",
        "        print(f\"Run {run} finished. Final visited states: {visited_states[-1]}\")\n",
        "        print(f\"Total intrinsic reward collected: {cumulative_intrinsic_rewards[-1]:.2f}\")\n",
        "\n",
        "    # Prepare data for plotting\n",
        "    # Ensure all lists are the same length\n",
        "    min_length = min(len(states) for states in all_visited_states)\n",
        "    all_visited_states = [states[:min_length] for states in all_visited_states]\n",
        "\n",
        "    # Convert to numpy array for easier calculations\n",
        "    visited_states_array = np.array(all_visited_states)\n",
        "\n",
        "    # Calculate mean and standard deviation\n",
        "    mean_visited_states = np.mean(visited_states_array, axis=0)\n",
        "    std_visited_states = np.std(visited_states_array, axis=0)\n",
        "\n",
        "    # Steps for x-axis\n",
        "    steps = list(range(min_length))\n",
        "\n",
        "    evaluation_data = {\n",
        "        \"all_visited_states\": all_visited_states,\n",
        "        \"mean_visited_states\": mean_visited_states,\n",
        "        \"std_visited_states\": std_visited_states,\n",
        "        \"steps\": steps,\n",
        "        \"all_intrinsic_rewards\": all_intrinsic_rewards,\n",
        "        \"all_steps\": all_steps\n",
        "    }\n",
        "\n",
        "    # Save the aggregated data\n",
        "    with open(\"evaluation_data_lp/aggregated_data.pkl\", \"wb\") as f:\n",
        "        pickle.dump(evaluation_data, f)\n",
        "\n",
        "    return evaluation_data\n",
        "\n",
        "def plot_state_coverage_lp(evaluation_data):\n",
        "    \"\"\"\n",
        "    Plot the average and variance of visited states over time for LP agent\n",
        "\n",
        "    Args:\n",
        "        evaluation_data: Dictionary with evaluation data\n",
        "    \"\"\"\n",
        "    steps = evaluation_data[\"steps\"]\n",
        "    mean_visited_states = evaluation_data[\"mean_visited_states\"]\n",
        "    std_visited_states = evaluation_data[\"std_visited_states\"]\n",
        "\n",
        "    # Create plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Plot mean with shaded standard deviation\n",
        "    plt.plot(steps, mean_visited_states, 'r-', linewidth=2, label='Mean Visited States')\n",
        "    plt.fill_between(steps,\n",
        "                     mean_visited_states - std_visited_states,\n",
        "                     mean_visited_states + std_visited_states,\n",
        "                     color='r', alpha=0.2, label='±1 Standard Deviation')\n",
        "\n",
        "    plt.xlabel('Exploration Steps')\n",
        "    plt.ylabel('Number of Visited States')\n",
        "    plt.title('State Coverage During Exploration with Learning Progress Curiosity')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig('state_coverage_lp.png')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # Also plot the individual runs\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Plot each individual run\n",
        "    all_visited_states = evaluation_data[\"all_visited_states\"]\n",
        "    for i, states in enumerate(all_visited_states):\n",
        "        plt.plot(steps, states, label=f'Run {i+1}')\n",
        "\n",
        "    # Also plot the mean\n",
        "    plt.plot(steps, mean_visited_states, 'k-', linewidth=3, label='Mean')\n",
        "\n",
        "    plt.xlabel('Exploration Steps')\n",
        "    plt.ylabel('Number of Visited States')\n",
        "    plt.title('State Coverage For Individual Runs with Learning Progress Curiosity')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig('state_coverage_individual_lp.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the aggregate intrinsic rewards\n",
        "    if \"all_intrinsic_rewards\" in evaluation_data and \"all_steps\" in evaluation_data:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Plot each run's intrinsic rewards\n",
        "        all_intrinsic_rewards = evaluation_data[\"all_intrinsic_rewards\"]\n",
        "        all_steps = evaluation_data[\"all_steps\"]\n",
        "\n",
        "        # Plot each run\n",
        "        for i, (steps, rewards) in enumerate(zip(all_steps, all_intrinsic_rewards)):\n",
        "            # Only plot until the last valid step\n",
        "            plt.plot(steps, rewards, label=f'Run {i+1}')\n",
        "\n",
        "        plt.xlabel('Exploration Steps')\n",
        "        plt.ylabel('Cumulative Intrinsic Reward')\n",
        "        plt.title('Cumulative Intrinsic Rewards Across All Runs')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.savefig('aggregate_intrinsic_rewards_lp.png')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    print(\"Plots generated and saved as 'state_coverage_lp.png', 'state_coverage_individual_lp.png', and 'aggregate_intrinsic_rewards_lp.png'\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Parameters for evaluation\n",
        "    NUM_RUNS = 10  # Number of runs for evaluation\n",
        "    MAX_STEPS = 50000\n",
        "    LOG_FREQUENCY = 100\n",
        "    FRAME_CAPTURE_FREQUENCY = 10  # Capture a frame every 10 steps\n",
        "\n",
        "    # Make sure the environment is registered\n",
        "    from gymnasium.envs.registration import register\n",
        "    try:\n",
        "        register(\n",
        "            id=\"MiniWorld-Maze_env-v0\",\n",
        "            entry_point=\"n_shape:MazeEnv\",\n",
        "        )\n",
        "    except:\n",
        "        # Environment might already be registered\n",
        "        pass\n",
        "\n",
        "    # Run evaluation and collect data on state coverage\n",
        "    evaluation_data = evaluate_state_coverage_lp(NUM_RUNS, MAX_STEPS, LOG_FREQUENCY, FRAME_CAPTURE_FREQUENCY)\n",
        "\n",
        "    # Plot the results\n",
        "    plot_state_coverage_lp(evaluation_data)\n",
        "\n",
        "    # Print summarized results\n",
        "    final_mean = evaluation_data[\"mean_visited_states\"][-1]\n",
        "    final_std = evaluation_data[\"std_visited_states\"][-1]\n",
        "\n",
        "    # Calculate average total intrinsic reward\n",
        "    total_intrinsic_rewards = [rewards[-1] for rewards in evaluation_data[\"all_intrinsic_rewards\"]]\n",
        "    avg_intrinsic_reward = sum(total_intrinsic_rewards) / len(total_intrinsic_rewards)\n",
        "    std_intrinsic_reward = np.std(total_intrinsic_rewards)\n",
        "\n",
        "    print(f\"\\n--- Evaluation Results for Learning Progress Curiosity ---\")\n",
        "    print(f\"Total runs: {NUM_RUNS}\")\n",
        "    print(f\"Average final state coverage: {final_mean:.1f} ± {final_std:.1f}\")\n",
        "    print(f\"Average total intrinsic reward: {avg_intrinsic_reward:.2f} ± {std_intrinsic_reward:.2f}\")\n",
        "    print(f\"Detailed data saved in the 'evaluation_data_lp' directory\")\n",
        "    print(f\"Exploration GIFs saved in the 'visualization_lp' directory\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
